{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk import FreqDist\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from transformers import pipeline\n",
    "\n",
    "from nltk.sentiment import sentiment_analyzer, vader, SentimentAnalyzer, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from LA Subreddit ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_submissions = 'https://api.pushshift.io/reddit/search/submission'\n",
    "url_comments = 'https://api.pushshift.io/reddit/search/comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_posts_cols = ['all_awardings', 'allow_live_comments', 'author',\n",
    "       'author_flair_css_class', 'author_flair_richtext', 'author_flair_text',\n",
    "       'author_flair_type', 'author_fullname', 'author_patreon_flair',\n",
    "       'author_premium', 'awarders', 'can_mod_post', 'contest_mode',\n",
    "       'created_utc', 'domain', 'full_link', 'gildings', 'id',\n",
    "       'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video',\n",
    "       'link_flair_background_color', 'link_flair_richtext', 'link_flair_text',\n",
    "       'link_flair_text_color', 'link_flair_type', 'locked', 'media_only',\n",
    "       'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n",
    "       'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
    "       'removed_by_category', 'retrieved_on', 'score', 'selftext',\n",
    "       'send_replies', 'spoiler', 'steward_reports', 'stickied', 'subreddit',\n",
    "       'subreddit_id', 'subreddit_subscribers', 'subreddit_type', 'thumbnail',\n",
    "       'title', 'total_awards_received', 'url', 'whitelist_status', 'wls',\n",
    "       'author_flair_background_color', 'author_flair_text_color',\n",
    "       'link_flair_css_class', 'author_flair_template_id', 'crosspost_parent',\n",
    "       'crosspost_parent_list', 'post_hint', 'preview', 'thumbnail_height',\n",
    "       'thumbnail_width', 'media_metadata', 'media', 'media_embed',\n",
    "       'secure_media', 'secure_media_embed', 'suggested_sort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n"
     ]
    }
   ],
   "source": [
    "LA_posts_df = pd.DataFrame(columns = LA_posts_cols)\n",
    "before_params = [1577865600] # first timestamp is equivalent to midnight PT on 01/01/2020 \n",
    "for i in range(1, 21):\n",
    "    resp = requests.get(url_submissions, params = {'subreddit': 'LosAngeles','size': 100,'before': before_params[-1]})\n",
    "    try: \n",
    "        batch = pd.DataFrame(resp.json()['data'])\n",
    "        LA_posts_df = pd.concat([LA_posts_df, batch], axis = 0 )\n",
    "        before_params.append(batch['created_utc'].min())\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900, 80)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA_posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anyone in the LA area want to hang tomorrow?[r...</td>\n",
       "      <td>Anyone in the LA area want to hang tomorrow?</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Father’s 1985 Toyota MR2 stolen this AM from S...</td>\n",
       "      <td>Father’s 1985 Toyota MR2 stolen this AM from SFV</td>\n",
       "      <td>My dad’s MR2 was stolen from the front of his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why does Downtown Los Angeles have so many lux...</td>\n",
       "      <td>Why does Downtown Los Angeles have so many lux...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mt baldythinking of heading there tomorrow,cha...</td>\n",
       "      <td>Mt baldy</td>\n",
       "      <td>thinking of heading there tomorrow,chains need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20//20</td>\n",
       "      <td>20//20</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title_selftext  \\\n",
       "0  Anyone in the LA area want to hang tomorrow?[r...   \n",
       "1  Father’s 1985 Toyota MR2 stolen this AM from S...   \n",
       "2  Why does Downtown Los Angeles have so many lux...   \n",
       "3  Mt baldythinking of heading there tomorrow,cha...   \n",
       "4                                             20//20   \n",
       "\n",
       "                                               title  \\\n",
       "0       Anyone in the LA area want to hang tomorrow?   \n",
       "1   Father’s 1985 Toyota MR2 stolen this AM from SFV   \n",
       "2  Why does Downtown Los Angeles have so many lux...   \n",
       "3                                           Mt baldy   \n",
       "4                                             20//20   \n",
       "\n",
       "                                            selftext  \n",
       "0                                          [removed]  \n",
       "1  My dad’s MR2 was stolen from the front of his ...  \n",
       "2                                                     \n",
       "3  thinking of heading there tomorrow,chains need...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA_posts_df['title_selftext'] = LA_posts_df['title']+LA_posts_df['selftext']\n",
    "LA_posts_df[['title_selftext', 'title', 'selftext']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_comments_cols = ['all_awardings', 'associated_award', 'author',\n",
    "       'author_flair_background_color', 'author_flair_css_class',\n",
    "       'author_flair_richtext', 'author_flair_template_id',\n",
    "       'author_flair_text', 'author_flair_text_color', 'author_flair_type',\n",
    "       'author_fullname', 'author_patreon_flair', 'author_premium', 'awarders',\n",
    "       'body', 'collapsed_because_crowd_control', 'created_utc', 'gildings',\n",
    "       'id', 'is_submitter', 'link_id', 'locked', 'no_follow', 'parent_id',\n",
    "       'permalink', 'retrieved_on', 'score', 'send_replies', 'steward_reports',\n",
    "       'stickied', 'subreddit', 'subreddit_id', 'total_awards_received',\n",
    "       'distinguished']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_comments_df = pd.DataFrame(columns = LA_comments_cols)\n",
    "before_params = [1577865600] # first timestamp is equivalent to midnight PT on 01/01/2020 \n",
    "for i in range(1, 21):\n",
    "    resp = requests.get(url_comments, params = {'subreddit': 'LosAngeles','size': 100,'before': before_params[-1]})\n",
    "    try: \n",
    "        batch = pd.DataFrame(resp.json()['data'])\n",
    "        LA_comments_df = pd.concat([LA_comments_df, batch], axis = 0 )\n",
    "        before_params.append(batch['created_utc'].min())\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 35)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA_comments_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from BOS Subreddit ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_posts_cols = ['all_awardings', 'allow_live_comments', 'author',\n",
    "       'author_flair_background_color', 'author_flair_css_class',\n",
    "       'author_flair_richtext', 'author_flair_text', 'author_flair_text_color',\n",
    "       'author_flair_type', 'author_fullname', 'author_patreon_flair',\n",
    "       'author_premium', 'awarders', 'can_mod_post', 'contest_mode',\n",
    "       'created_utc', 'domain', 'full_link', 'gildings', 'id',\n",
    "       'is_crosspostable', 'is_meta', 'is_original_content',\n",
    "       'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video',\n",
    "       'link_flair_background_color', 'link_flair_richtext',\n",
    "       'link_flair_text_color', 'link_flair_type', 'locked', 'media_only',\n",
    "       'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n",
    "       'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
    "       'retrieved_on', 'score', 'selftext', 'send_replies', 'spoiler',\n",
    "       'steward_reports', 'stickied', 'subreddit', 'subreddit_id',\n",
    "       'subreddit_subscribers', 'subreddit_type', 'thumbnail', 'title',\n",
    "       'total_awards_received', 'url', 'whitelist_status', 'wls',\n",
    "       'link_flair_css_class', 'link_flair_template_id', 'link_flair_text',\n",
    "       'author_flair_template_id', 'post_hint', 'preview', 'thumbnail_height',\n",
    "       'thumbnail_width', 'removed_by_category', 'media', 'secure_media',\n",
    "       'crosspost_parent', 'crosspost_parent_list', 'media_embed',\n",
    "       'secure_media_embed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n"
     ]
    }
   ],
   "source": [
    "BOS_posts_df = pd.DataFrame(columns = BOS_posts_cols)\n",
    "before_params = [1577872800] # first timestamp is equivalent to midnight ET on 01/01/2020 \n",
    "for i in range(1, 21):\n",
    "    resp = requests.get(url_submissions, params = {'subreddit': 'boston','size': 100,'before': before_params[-1]})\n",
    "    try: \n",
    "        batch = pd.DataFrame(resp.json()['data'])\n",
    "        BOS_posts_df = pd.concat([BOS_posts_df, batch], axis = 0 )\n",
    "        before_params.append(batch['created_utc'].min())\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900, 76)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS_posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fireworks from the Seaport</td>\n",
       "      <td>Fireworks from the Seaport</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Missed the last train home from north station ...</td>\n",
       "      <td>Missed the last train home from north station :(</td>\n",
       "      <td>What are my options and I can’t afford an Uber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Moving To BostonHello people of Boston I’m fro...</td>\n",
       "      <td>Moving To Boston</td>\n",
       "      <td>Hello people of Boston I’m from the south shor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$71 to go from the North End to the South End</td>\n",
       "      <td>$71 to go from the North End to the South End</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boston First Night Family Fireworks</td>\n",
       "      <td>Boston First Night Family Fireworks</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title_selftext  \\\n",
       "0                         Fireworks from the Seaport   \n",
       "1  Missed the last train home from north station ...   \n",
       "2  Moving To BostonHello people of Boston I’m fro...   \n",
       "3      $71 to go from the North End to the South End   \n",
       "4                Boston First Night Family Fireworks   \n",
       "\n",
       "                                              title  \\\n",
       "0                        Fireworks from the Seaport   \n",
       "1  Missed the last train home from north station :(   \n",
       "2                                  Moving To Boston   \n",
       "3     $71 to go from the North End to the South End   \n",
       "4               Boston First Night Family Fireworks   \n",
       "\n",
       "                                            selftext  \n",
       "0                                                     \n",
       "1  What are my options and I can’t afford an Uber...  \n",
       "2  Hello people of Boston I’m from the south shor...  \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS_posts_df['title_selftext'] = BOS_posts_df['title']+BOS_posts_df['selftext']\n",
    "BOS_posts_df[['title_selftext', 'title', 'selftext']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_comments_cols = ['all_awardings', 'associated_award', 'author',\n",
    "       'author_flair_background_color', 'author_flair_css_class',\n",
    "       'author_flair_richtext', 'author_flair_template_id',\n",
    "       'author_flair_text', 'author_flair_text_color', 'author_flair_type',\n",
    "       'author_fullname', 'author_patreon_flair', 'author_premium', 'awarders',\n",
    "       'body', 'collapsed_because_crowd_control', 'created_utc', 'gildings',\n",
    "       'id', 'is_submitter', 'link_id', 'locked', 'no_follow', 'parent_id',\n",
    "       'permalink', 'retrieved_on', 'score', 'send_replies', 'steward_reports',\n",
    "       'stickied', 'subreddit', 'subreddit_id', 'total_awards_received',\n",
    "       'author_cakeday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n"
     ]
    }
   ],
   "source": [
    "BOS_comments_df = pd.DataFrame(columns = BOS_comments_cols)\n",
    "before_params = [1577865600] # first timestamp is equivalent to midnight PT on 01/01/2020 \n",
    "for i in range(1, 21):\n",
    "    resp = requests.get(url_comments, params = {'subreddit': 'boston','size': 100,'before': before_params[-1]})\n",
    "    try: \n",
    "        batch = pd.DataFrame(resp.json()['data'])\n",
    "        BOS_comments_df = pd.concat([BOS_comments_df, batch], axis = 0 )\n",
    "        before_params.append(batch['created_utc'].min())\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900, 35)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS_comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_comments_df['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVING DATA AS PULLED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_comments_df.to_csv('bos_com.csv')\n",
    "BOS_posts_df.to_csv('bos_pos.csv')\n",
    "LA_comments_df.to_csv('la_com.csv')\n",
    "LA_posts_df.to_csv('la_pos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RETRIEVING DATA AS PULLED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_comments_df = pd.read_csv('bos_com.csv')\n",
    "BOS_posts_df = pd.read_csv('bos_pos.csv')\n",
    "LA_comments_df = pd.read_csv('la_com.csv')\n",
    "LA_posts_df = pd.read_csv('la_pos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')\n",
    "additions = [\"!\",\"?\",\".\",\":\",\";\", \",\", \"\\'\", \"\\\"\",\"*\", \"'\", '\"', \"[\", ']', '(', \")\", '’']\n",
    "stops.extend(additions)\n",
    "#len(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_More to be done here later_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_comments_corpus = ''\n",
    "for i in range(len(BOS_comments_df)):\n",
    "    BOS_comments_corpus+=(BOS_comments_df.iloc[i,list(BOS_comments_df.columns).index('body')])\n",
    "\n",
    "LA_comments_corpus = ''\n",
    "for i in range(len(LA_comments_df)):\n",
    "    LA_comments_corpus+=(LA_comments_df.iloc[i,list(LA_comments_df.columns).index('body')])\n",
    "\n",
    "BOS_posts_corpus = ''\n",
    "for i in range(len(BOS_posts_df)):\n",
    "    BOS_posts_corpus+=(BOS_posts_df.iloc[i,list(BOS_posts_df.columns).index('title_selftext')])\n",
    "    \n",
    "LA_posts_corpus = ''\n",
    "for i in range(len(LA_posts_df)):\n",
    "    LA_posts_corpus+=str(LA_posts_df.iloc[i,list(LA_posts_df.columns).index('title_selftext')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer and dumb model on just comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([BOS_comments_df['body'], LA_comments_df['body']], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([BOS_comments_df['body'], LA_comments_df['body']], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate([np.zeros_like(BOS_comments_df['body']), np.ones_like(LA_comments_df['body'])]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english', lowercase = True, max_features=10_000)\n",
    "train_X_vec = cvec.fit_transform(train_X)\n",
    "test_X_vec = cvec.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(train_X_vec, train_y)\n",
    "mnb.score(train_X_vec, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.score(test_X_vec, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing location-specific references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_mdl = spacy.load('en_core_web_md')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I miss \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Boston\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " sometimes</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nlp(str('I miss Boston sometimes')), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(BOS_comments_corpus + BOS_posts_corpus)\n",
    "bos_loc_gpe = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ in ['LOC', 'GPE']:\n",
    "        bos_loc_gpe.append(ent.text)\n",
    "\n",
    "BOS_locations = list(set(bos_loc_gpe))\n",
    "BOS_locations.append('Harvard') # adding some which were not detected by Spacy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "poppers =  ['Airbnb', 'Aquarium', 'Atlanta', 'Austin', 'Australia', 'Automobiles',           \n",
    "            'Billy','Black', 'Brno', \n",
    "           'California', 'Canada','Carolinas','Central','Charlotte', 'Chicago','Chlamydia', 'Comcast','Czech Republic',\n",
    "           'DC','Davis','Dec 21.Found', 'Detroit', 'Downtown', 'Dubai',\n",
    "           'East', 'England',\n",
    "           'Florida','Hong Kong','Hotel','Houston','Hyde Park',\n",
    "           'Jan', 'Japan', 'Kansas', 'Kansas City', 'Karaoke',\n",
    "           'Meridian', 'Miami', 'Midwest','Minecraft!Just','Mississippi','Mueller','Mumbai',\n",
    "            'NH','NY','Naples','Nashville',\n",
    "           'States', 'Storrow', 'Suffolk County', 'Sweden','Syracuse', 'Syria', \n",
    "           'Tennessee','Texas',\"The Combat Zoneit's\",'Toronto',\n",
    "            'U.K.','US','USA','Venmo', 'Vermont', 'Washington', 'Washington DC']\n",
    "for popper in poppers:\n",
    "    BOS_locations.remove(popper) #removing some which are not BOS specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOS_locations.sort()\n",
    "#BOS_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(LA_comments_corpus + LA_posts_corpus)\n",
    "la_loc_gpe = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ in ['LOC', 'GPE']:\n",
    "        la_loc_gpe.append(ent.text)\n",
    "\n",
    "LA_locations = list(set(la_loc_gpe))\n",
    "la_adds = [\"Westside\", \"Los\", \"Angeles\"]\n",
    "for add in la_adds:\n",
    "    LA_locations.append(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_poppers = ['Airbnb','Alabama', 'America', 'Alexandria', 'AmericaIt', 'Canada', 'Bernie', 'Building', 'California',\n",
    "              'Central America', 'Chicago', 'Cincinnati', 'Colorado', 'Costa Rica', 'DC', 'Dallas', 'Denver','East',\n",
    "              'Idaho', 'Indiana','Iowa','Japan', 'Korea', 'Las Vegas', 'London',\n",
    "              'Midwest', 'Mueller', 'Texas', 'US', 'USA', 'Vermont', 'Washington', 'Washington DC', 'the United States',\n",
    "              'Vancouver', 'Vegas', 'boston',  'Tokyo','Tsunamis', 'U.S.','United State','United States Of America']\n",
    "for popper in la_poppers:\n",
    "    LA_locations.remove(popper) #removing some which are not LA specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LA_locations.sort()\n",
    "#LA_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_comments_df['body_np'] = LA_comments_df['body']\n",
    "for loc in LA_locations:\n",
    "    LA_comments_df['body_np'] = LA_comments_df['body_np'].apply(lambda x: x.replace(loc, \"PLACE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_comments_df['body_np'] = BOS_comments_df['body']\n",
    "for loc in BOS_locations:\n",
    "    BOS_comments_df['body_np'] = BOS_comments_df['body_np'].apply(lambda x: x.replace(loc, \"PLACE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVING DATA WITHOUT LOCATION REFERENCES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_comments_df.to_csv('bos_com.csv')\n",
    "BOS_posts_df.to_csv('bos_pos.csv')\n",
    "LA_comments_df.to_csv('la_com.csv')\n",
    "LA_posts_df.to_csv('la_pos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHAT ABOUT WITHOUT LOCATION NAMES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([BOS_comments_df['body_np'], LA_comments_df['body_np']], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate([np.zeros_like(BOS_comments_df['body_np']), np.ones_like(LA_comments_df['body_np'])]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english', lowercase = True, max_features=10_000)\n",
    "train_X_vec = cvec.fit_transform(train_X)\n",
    "test_X_vec = cvec.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(train_X_vec, train_y)\n",
    "mnb.score(train_X_vec, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW ON ACTUAL POSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_posts_df['title_selftext_np'] = LA_posts_df['title_selftext']\n",
    "for loc in LA_locations:\n",
    "    LA_posts_df['title_selftext_np'] = LA_posts_df['title_selftext_np'].apply(lambda x: str(x).replace(loc, \"PLACE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_posts_df['title_selftext_np'] = BOS_posts_df['title_selftext']\n",
    "for loc in BOS_locations:\n",
    "    BOS_posts_df['title_selftext_np'] = BOS_posts_df['title_selftext_np'].apply(lambda x: x.replace(loc, \"PLACE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([BOS_posts_df['title_selftext_np'].dropna(), LA_posts_df['title_selftext_np'].dropna()], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones_like(LA_posts_df['title_selftext_np'].dropna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate([np.zeros_like(BOS_posts_df['title_selftext_np'].dropna()), np.ones_like(LA_posts_df['title_selftext_np'].dropna())]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english', lowercase = True, ngram_range=(1,2))#max_features=10_000)\n",
    "train_X_vec = cvec.fit_transform(train_X)\n",
    "test_X_vec = cvec.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(train_X_vec, train_y)\n",
    "mnb.score(train_X_vec, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.score(test_X_vec, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POSTS AND COMMENTS WITH LOCATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([BOS_posts_df['title_selftext'].dropna(), BOS_comments_df['body'].dropna(), LA_posts_df['title_selftext'].dropna(), LA_comments_df['body'].dropna()], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate([np.zeros_like(BOS_posts_df['title_selftext'].dropna()),\n",
    "                    np.zeros_like(BOS_comments_df['body'].dropna()),\n",
    "                    np.ones_like(LA_posts_df['title_selftext'].dropna()),\n",
    "                    np.ones_like(LA_comments_df['body'].dropna())]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english', lowercase = True, ngram_range=(1,2))#max_features=10_000)\n",
    "train_X_vec = cvec.fit_transform(train_X)\n",
    "test_X_vec = cvec.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4974974974974975"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.506"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968968968968969"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(train_X_vec, train_y)\n",
    "mnb.score(train_X_vec, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.774"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(test_X_vec, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW ON POSTS and COMMENTS with no LOC GPS references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([BOS_posts_df['title_selftext_np'].dropna(), BOS_comments_df['body_np'].dropna(), LA_posts_df['title_selftext_np'].dropna(), LA_comments_df['body_np'].dropna()], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3997,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate([np.zeros_like(BOS_posts_df['title_selftext_np'].dropna()),\n",
    "                    np.zeros_like(BOS_comments_df['body_np'].dropna()),\n",
    "                    np.ones_like(LA_posts_df['title_selftext_np'].dropna()),\n",
    "                    np.ones_like(LA_comments_df['body_np'].dropna())]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3997,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english', lowercase = True, ngram_range=(1,2))#max_features=10_000)\n",
    "train_X_vec = cvec.fit_transform(train_X)\n",
    "test_X_vec = cvec.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5028361695028362"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9669669669669669"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(train_X_vec, train_y)\n",
    "mnb.score(train_X_vec, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(test_X_vec, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(mnb, test_X_vec, test_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[[i for i in range(len(mnb.predict(test_X_vec))) if mnb.predict(test_X_vec[i]) == 1 and test_y[i] ==0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec = cvec.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score of 0.6767 achieved with alpha = 0.2222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  50 out of  50 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "params = {'alpha': np.linspace(0,1,10)}\n",
    "mnb_gs = GridSearchCV(mnb, params, n_jobs = 5, verbose = 1, cv = 5)\n",
    "\n",
    "mnb_gs.fit(X_vec, y)\n",
    "print(f\"Best CV score of {round(mnb_gs.best_score_,4)} achieved with alpha = {round(mnb_gs.best_params_['alpha'],4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 900 candidates, totalling 4500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:   52.3s\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=5)]: Done 3190 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=5)]: Done 4040 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=5)]: Done 4500 out of 4500 | elapsed: 11.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6734937421777222"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec_mnb_pipe = Pipeline([('cvec', CountVectorizer()), \n",
    "                          ('mnb', MultinomialNB())])\n",
    "\n",
    "cvec_mnb_params = {'cvec__stop_words': ['english', None],\n",
    "                    'cvec__ngram_range': [(1,1), (1,2), (2,2), (1,3), (3,3)],\n",
    "                  'cvec__max_features': [5_000, 10_000, None],\n",
    "                    'cvec__max_df': [0.7, 0.8, 0.9, None],\n",
    "                  'mnb__alpha': np.linspace(0,1,10)}\n",
    "\n",
    "cvec_mnb_gs = GridSearchCV(cvec_mnb_pipe, cvec_mnb_params, cv = 5, verbose = 1, n_jobs = 5)\n",
    "cvec_mnb_gs.fit(X,y)\n",
    "cvec_mnb_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.7,\n",
       " 'cvec__max_features': None,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__stop_words': 'english',\n",
       " 'mnb__alpha': 0.2222222222222222}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec_mnb_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1200 candidates, totalling 6000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:   59.1s\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=5)]: Done 3190 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=5)]: Done 4040 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=5)]: Done 4990 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=5)]: Done 6000 out of 6000 | elapsed: 13.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.674241551939925"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mnb_pipe = Pipeline([('tfidf', TfidfVectorizer()), \n",
    "                          ('mnb', MultinomialNB())])\n",
    "\n",
    "tfidf_mnb_params = {'tfidf__stop_words': ['english', None],\n",
    "                    'tfidf__ngram_range': [(1,1), (1,2), (2,2), (1,3), (3,3)],\n",
    "                  'tfidf__max_features': [5_000, 10_000, None],\n",
    "                    'tfidf__max_df': [0.7, 0.8, 0.9, None],\n",
    "                  'mnb__alpha': np.linspace(0,1,10)}\n",
    "\n",
    "tfidf_mnb_gs = GridSearchCV(tfidf_mnb_pipe, tfidf_mnb_params, cv = 5, verbose = 1, n_jobs = 5)\n",
    "tfidf_mnb_gs.fit(X,y)\n",
    "tfidf_mnb_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mnb__alpha': 0.1111111111111111,\n",
       " 'tfidf__max_df': 0.7,\n",
       " 'tfidf__max_features': None,\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__stop_words': 'english'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mnb_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MESSING WITH SENTIMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "sia.polarity_scores(BOS_comments_corpus_noprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(LA_comments_corpus_noprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(BOS_posts_corpus+BOS_comments_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(LA_posts_corpus+LA_comments_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_posts_df['sia_neg'] = BOS_posts_df['title_selftext_np'].apply(lambda x: sia.polarity_scores(x)['neg'])\n",
    "BOS_posts_df['sia_pos'] = BOS_posts_df['title_selftext_np'].apply(lambda x: sia.polarity_scores(x)['pos'])\n",
    "LA_posts_df['sia_neg'] = LA_posts_df['title_selftext_np'].apply(lambda x: sia.polarity_scores(x)['neg'])\n",
    "LA_posts_df['sia_pos'] = LA_posts_df['title_selftext_np'].apply(lambda x: sia.polarity_scores(x)['pos'])\n",
    "\n",
    "BOS_comments_df['sia_neg'] = BOS_comments_df['body_np'].apply(lambda x: sia.polarity_scores(x)['neg'])\n",
    "BOS_comments_df['sia_pos'] = BOS_comments_df['body_np'].apply(lambda x: sia.polarity_scores(x)['pos'])\n",
    "LA_comments_df['sia_neg'] = LA_comments_df['body_np'].apply(lambda x: sia.polarity_scores(x)['neg'])\n",
    "LA_comments_df['sia_pos'] = LA_comments_df['body_np'].apply(lambda x: sia.polarity_scores(x)['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('strength of negative sentiment in posts')\n",
    "plt.hist(BOS_posts_df['sia_neg'], histtype = 'step', label = 'Boston')\n",
    "plt.hist(LA_posts_df['sia_neg'], histtype = 'step', label = 'LA')\n",
    "plt.axvline(sia.polarity_scores(BOS_posts_corpus)['neg'], ls = '--', lw = 1, color = 'blue')\n",
    "plt.axvline(sia.polarity_scores(LA_posts_corpus)['neg'], ls = '--', lw = 1, color = 'orange')\n",
    "plt.legend();\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('strength of positive sentiment in posts')\n",
    "plt.hist(BOS_posts_df['sia_pos'], histtype = 'step', label = 'Boston')\n",
    "plt.hist(LA_posts_df['sia_pos'], histtype = 'step',label = 'LA')\n",
    "plt.axvline(sia.polarity_scores(BOS_posts_corpus)['pos'], ls = '--', lw = 1,  color = 'blue')\n",
    "plt.axvline(sia.polarity_scores(LA_posts_corpus)['pos'], ls = '--',lw = 1, color = 'orange')\n",
    "plt.legend();\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('strength of negative sentiment in comments')\n",
    "plt.hist(BOS_comments_df['sia_neg'], histtype = 'step', label = 'Boston')\n",
    "plt.hist(LA_comments_df['sia_neg'], histtype = 'step', label = 'LA')\n",
    "plt.axvline(sia.polarity_scores(BOS_comments_corpus)['neg'], ls = '--', lw = 1, color = 'blue')\n",
    "plt.axvline(sia.polarity_scores(LA_comments_corpus)['neg'], ls = '--', lw = 1, color = 'orange')\n",
    "plt.legend();\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('strength of positive sentiment in comments')\n",
    "plt.hist(BOS_comments_df['sia_pos'], histtype = 'step', label = 'Boston')\n",
    "plt.hist(LA_comments_df['sia_pos'], histtype = 'step',label = 'LA')\n",
    "plt.axvline(sia.polarity_scores(BOS_comments_corpus)['pos'], ls = '--', lw = 1,  color = 'blue')\n",
    "plt.axvline(sia.polarity_scores(LA_comments_corpus)['pos'], ls = '--',lw = 1, color = 'orange')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertModel: ['classifier', 'pre_classifier', 'dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_38']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997004866600037}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti = pipeline('sentiment-analysis')\n",
    "senti('I have the worst boyfriend on the planet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_posts_df['senti_pn'] = BOS_posts_df['title_selftext_np'].apply(lambda x: senti(x[:512])[0]['label'])\n",
    "BOS_posts_df['senti_score'] = BOS_posts_df['title_selftext_np'].apply(lambda x: senti(x[:512])[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_posts_df['senti_pn'] = LA_posts_df['title_selftext_np'].apply(lambda x: senti(str(x)[:512])[0]['label'])\n",
    "LA_posts_df['senti_score'] = LA_posts_df['title_selftext_np'].apply(lambda x: senti(str(x)[:512])[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_comments_df['senti_pn'] = BOS_comments_df['body_np'].apply(lambda x: senti(str(x)[:512])[0]['label'])\n",
    "BOS_comments_df['senti_score'] = BOS_comments_df['body_np'].apply(lambda x: senti(str(x)[:512])[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_comments_df['senti_pn'] = LA_comments_df['body_np'].apply(lambda x: senti(str(x)[:512])[0]['label'])\n",
    "LA_comments_df['senti_score'] = LA_comments_df['body_np'].apply(lambda x: senti(str(x)[:512])[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.347"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BOS_posts_df[BOS_posts_df['senti_pn'] == 'POSITIVE'])/len(BOS_posts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LA_posts_df[LA_posts_df['senti_pn'] == 'POSITIVE'])/len(LA_posts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.561"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BOS_posts_df[BOS_posts_df['senti_pn'] == 'NEGATIVE'])/len(BOS_posts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.572"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LA_posts_df[LA_posts_df['senti_pn'] == 'NEGATIVE'])/len(LA_posts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sia_neg</th>\n",
       "      <th>sia_pos</th>\n",
       "      <th>senti_pn</th>\n",
       "      <th>senti_score</th>\n",
       "      <th>senti_pn_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.999072</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.999253</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.137</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.768650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.998603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sia_neg  sia_pos  senti_pn  senti_score  senti_pn_bin\n",
       "0    0.000    0.000  POSITIVE     0.999072             1\n",
       "1    0.109    0.000  NEGATIVE    -0.999253            -1\n",
       "2    0.020    0.137   NEUTRAL     0.000000             0\n",
       "3    0.000    0.000  POSITIVE     0.768650             1\n",
       "4    0.000    0.000  POSITIVE     0.998603             1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS_posts_df.head().iloc[:,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_posts_df['senti_score'].fillna(0, inplace = True)\n",
    "LA_comments_df['senti_score'].fillna(0, inplace = True)\n",
    "BOS_posts_df['senti_score'].fillna(0, inplace = True)\n",
    "BOS_comments_df['senti_score'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_posts_df['senti_pn'].fillna('NEUTRAL', inplace = True)\n",
    "LA_comments_df['senti_pn'].fillna('NEUTRAL', inplace = True)\n",
    "BOS_posts_df['senti_pn'].fillna('NEUTRAL', inplace = True)\n",
    "BOS_comments_df['senti_pn'].fillna('NEUTRAL', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_posts_df['senti_pn_bin'] = BOS_posts_df['senti_pn'].map({'POSITIVE': 1, 'NEUTRAL': 0, 'NEGATIVE': -1})\n",
    "BOS_comments_df['senti_pn_bin'] = BOS_comments_df['senti_pn'].map({'POSITIVE': 1, 'NEUTRAL': 0, 'NEGATIVE': -1})\n",
    "LA_posts_df['senti_pn_bin'] = LA_posts_df['senti_pn'].map({'POSITIVE': 1, 'NEUTRAL': 0, 'NEGATIVE': -1})\n",
    "LA_comments_df['senti_pn_bin'] = LA_comments_df['senti_pn'].map({'POSITIVE': 1, 'NEUTRAL': 0, 'NEGATIVE': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_posts_df['senti_score'] = LA_posts_df['senti_pn_bin']*LA_posts_df['senti_score']\n",
    "LA_comments_df['senti_score']= LA_comments_df['senti_pn_bin']*LA_comments_df['senti_score']\n",
    "BOS_posts_df['senti_score'] = BOS_posts_df['senti_pn_bin']*BOS_posts_df['senti_score']\n",
    "BOS_comments_df['senti_score'] = BOS_comments_df['senti_pn_bin']*BOS_comments_df['senti_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([BOS_posts_df[['sia_neg', 'sia_pos', 'senti_score']], BOS_comments_df[['sia_neg', 'sia_pos', 'senti_score']],\n",
    "              LA_posts_df[['sia_neg', 'sia_pos', 'senti_score']], LA_comments_df[['sia_neg', 'sia_pos', 'senti_score']]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate([np.zeros_like(BOS_posts_df.index),\n",
    "                    np.zeros_like(BOS_comments_df.index),\n",
    "                    np.ones_like(LA_posts_df.index),\n",
    "                    np.ones_like(LA_comments_df.index)]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X,train_y, test_y = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5223333333333333"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr = LogisticRegression()\n",
    "logr.fit(train_X, train_y)\n",
    "logr.score(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.518"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVING DATA WITH SENTIMENT SCORES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_comments_df.to_csv('bos_com.csv')\n",
    "BOS_posts_df.to_csv('bos_pos.csv')\n",
    "LA_comments_df.to_csv('la_com.csv')\n",
    "LA_posts_df.to_csv('la_pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPLORATIVE ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martalew/anaconda3/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>associated_award</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>...</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>body_np</th>\n",
       "      <th>sia_neg</th>\n",
       "      <th>sia_pos</th>\n",
       "      <th>senti_pn</th>\n",
       "      <th>senti_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, all_awardings, associated_award, author, author_flair_background_color, author_flair_css_class, author_flair_richtext, author_flair_template_id, author_flair_text, author_flair_text_color, author_flair_type, author_fullname, author_patreon_flair, author_premium, awarders, body, collapsed_because_crowd_control, created_utc, gildings, id, is_submitter, link_id, locked, no_follow, parent_id, permalink, retrieved_on, score, send_replies, steward_reports, stickied, subreddit, subreddit_id, total_awards_received, distinguished, author_cakeday, body_np, sia_neg, sia_pos, senti_pn, senti_score]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA_comments_df[LA_posts_df['media_only']=='True']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_tokens = word_tokenize(BOS_posts_corpus+BOS_comments_corpus)\n",
    "\n",
    "BOS_tokens_no_stops = [token for token in BOS_tokens if not token in stops]\n",
    "BOS_tokens_no_stops_np = [token.lower() for token in BOS_tokens_no_stops if not token in BOS_locations]\n",
    "\n",
    "FreqDist(BOS_tokens_no_stops_np).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_tokens = word_tokenize(LA_posts_corpus + LA_comments_corpus)\n",
    "\n",
    "LA_tokens_no_stops = [token for token in LA_tokens if not token in stops]\n",
    "LA_tokens_no_stops_np = [token.lower() for token in LA_tokens_no_stops if not token in LA_locations]\n",
    "\n",
    "[word for word, freq in FreqDist(LA_tokens_no_stops_np).most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stops = [word for word,_ in FreqDist(BOS_tokens_no_stops_np).most_common(100) if word in [word for word, freq in FreqDist(LA_tokens_no_stops_np).most_common(100)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops.extend(more_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA EXPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_comments_df.to_csv('bos_com.csv')\n",
    "BOS_posts_df.to_csv('bos_pos.csv')\n",
    "LA_comments_df.to_csv('la_com.csv')\n",
    "LA_posts_df.to_csv('la_pos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OTHER MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_X_vec, train_y)\n",
    "knn.score(train_X_vec, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(test_X_vec, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1,11):\n",
    "    knn = KNeighborsClassifier(k)\n",
    "    print(f\"k = {k} --> score = {round(cross_val_score(knn, X_vec, y).mean(),4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_mnb_pipe = Pipeline([('cvec', CountVectorizer()), \n",
    "                          ('mnb', MultinomialNB())])\n",
    "\n",
    "cvec_mnb_params = {'cvec__stop_words': ['english', None],\n",
    "                    'cvec__ngram_range': [(1,1), (1,2), (2,2), (1,3), (3,3)],\n",
    "                  'cvec__max_features': [5_000, 10_000, None],\n",
    "                    'cvec__max_df': [0.7, 0.8, 0.9],\n",
    "                  'mnb__alpha': np.linspace(0,1,10)}\n",
    "\n",
    "cvec_mnb_gs = GridSearchCV(cvec_mnb_pipe, cvec_mnb_params, cv = 5, verbose = 1, n_jobs = 5)\n",
    "cvec_mnb_gs.fit(X,y)\n",
    "cvec_mnb_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(train_X_vec, train_y)\n",
    "rfc.score(train_X_vec, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(test_X_vec, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'bootstrap': [True],\n",
    " 'ccp_alpha': np.logspace(0,1,10),\n",
    " 'criterion': ['gini'],\n",
    " 'max_depth': [2,5,10,15],\n",
    " 'min_samples_leaf': [1,2,3,4,5],\n",
    " 'n_estimators': [25,50,75,100,125]}\n",
    "\n",
    "rfc_gs = GridSearchCV(rfc, params, cv = 5, n_jobs = 5, verbose = 1)\n",
    "\n",
    "rfc_gs.fit(X_vec, y)\n",
    "rfc_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_mnb_pipe = Pipeline([('cvec', CountVectorizer()), \n",
    "                          ('mnb', MultinomialNB())])\n",
    "\n",
    "cvec_mnb_params = {'cvec__stop_words': ['english', None],\n",
    "                    'cvec__ngram_range': [(1,1), (1,2), (2,2), (1,3), (3,3)],\n",
    "                  'cvec__max_features': [5_000, 10_000, None],\n",
    "                    'cvec__max_df': [0.7, 0.8, 0.9],\n",
    "                  'mnb__alpha': np.linspace(0,1,10)}\n",
    "\n",
    "cvec_mnb_gs = GridSearchCV(cvec_mnb_pipe, cvec_mnb_params, cv = 5, verbose = 1, n_jobs = 5)\n",
    "cvec_mnb_gs.fit(X,y)\n",
    "cvec_mnb_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = XGBClassifier(use_label_encoder=False, eval_metric = 'logloss')\n",
    "xgbc.fit(train_X_vec, train_y)\n",
    "xgbc.score(train_X_vec, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc.score(test_X_vec, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
